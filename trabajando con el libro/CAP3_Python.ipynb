{"cells":[{"cell_type":"code","source":["# Example of expressivity and composability. We use the low-level RDD API for this, telling Spark how to do it.\ndataRDD=sc.parallelize([(\"Brooke\",20),(\"Denny\",31),(\"Jules\",30),(\"TD\",35),(\"Brooke\",25)])\nagesRDD=(dataRDD.map(lambda x: (x[0],(x[1],1))).reduceByKey(lambda x, y: (x[0]+y[0],x[1]+y[1])).map(lambda x: (x[0], x[1][0]/x[1][1])))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c10fcfb5-61e0-4eaf-9df7-8dab65f572b0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Now, we express the last query with high-level DSL operatos and the DF API, instructing Spark what to do.\n#######################################################################################################################\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import avg\n\n# Create a DF using SparkSession\nspark = (SparkSession\n        .builder\n        .appName(\"AuthorsAges\")\n        .getOrCreate())\n\n# Create a DF\ndata_df = spark.createDataFrame([(\"Brooke\",20),(\"Denny\",31),(\"Jules\",30),(\"TD\",35),(\"Brooke\",25)],[\"name\",\"age\"])\n\n# Group the same names together, aggregate their ages and compute an average\navg_df = data_df.groupBy(\"name\").agg(avg(\"age\"))\n\n# Show the results of the final execution\navg_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08a90357-0dbb-40d3-981d-2e4bc8b88355"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+--------+\n|  name|avg(age)|\n+------+--------+\n|Brooke|    22.5|\n| Denny|    31.0|\n| Jules|    30.0|\n|    TD|    35.0|\n+------+--------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+--------+\n|  name|avg(age)|\n+------+--------+\n|Brooke|    22.5|\n| Denny|    31.0|\n| Jules|    30.0|\n|    TD|    35.0|\n+------+--------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# DEFINING SCHEMAS.\n# Firsty, we are going to define a schema programmatically for a DF with three named columns, author, title, and pages; using the Spark DF API.\n################################################################################################################################################\n\nfrom pyspark.sql.types import *\nschema=StructType([StructField(\"author\",StringType(),False),StructField(\"title\",StringType(),False),StructField(\"pages\",IntegerType(),False)])\n\n# Secondly, we define the same schema using DDL (Data Definition Language)\nschema2=\"author STRING, title STRING, pages INT\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ede09b6-1467-4b54-ad6b-cba58073d6d4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Another example of the usage of schemas.\n##################################################################################################################################\n\nfrom pyspark.sql import SparkSession\n# Define the schema\nschema= \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n# Create our static data\ndata = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\"LinkedIN\"]],\n        [2, \"Brooke\", \"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\"LinkedIN\"]],\n        [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\"twitter\",\"FB\",\"LinkedIN\"]],\n        [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\",\"FB\"]], \n        [5, \"Matei\", \"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\"twitter\",\"FB\",\"LinkedIN\"]],\n        [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\",\"LinkedIN\"]]\n       ]\n# Main program\nif __name__==\"__main__\":\n    # Create a SparkSession\n    spark= (SparkSession\n           .builder\n           .appName(\"Example-3_6\")\n           .getOrCreate())\n    #Create a DF using the schema defined above\n    blogs_df=spark.createDataFrame(data,schema)\n    blogs_df.show()\n    print(blogs_df.printSchema())\n    blogs_df.schema# "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec3a801d-f8e4-4fe7-8e71-91cff19c6050"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+---------+-------+-----------------+---------+-----+--------------------+\n| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n+---+---------+-------+-----------------+---------+-----+--------------------+\n|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIN]|\n|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIN]|\n|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIN]|\n+---+---------+-------+-----------------+---------+-----+--------------------+\n\nroot\n |-- Id: integer (nullable = true)\n |-- First: string (nullable = true)\n |-- Last: string (nullable = true)\n |-- Url: string (nullable = true)\n |-- Published: string (nullable = true)\n |-- Hits: integer (nullable = true)\n |-- Campaigns: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\nNone\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---------+-------+-----------------+---------+-----+--------------------+\n| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n+---+---------+-------+-----------------+---------+-----+--------------------+\n|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIN]|\n|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIN]|\n|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIN]|\n+---+---------+-------+-----------------+---------+-----+--------------------+\n\nroot\n |-- Id: integer (nullable = true)\n |-- First: string (nullable = true)\n |-- Last: string (nullable = true)\n |-- Url: string (nullable = true)\n |-- Published: string (nullable = true)\n |-- Hits: integer (nullable = true)\n |-- Campaigns: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\nNone\n"]}}],"execution_count":0},{"cell_type":"code","source":["# ROWS\n#################################################################33\n\nfrom pyspark.sql import Row\nblog_row=Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\",\"LinkedIN\"])\nblog_row[1]\n\n# Row objects can be used to create DF if you need them for quick interactivity and exploration.\nrows=[Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\",\"CA\")]\nauthors_df=spark.createDataFrame(rows,[\"Authos\",\"State\"])\nauthors_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a1491c9-9c7c-4e56-bff2-7e861a395458"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+-----+\n|       Authos|State|\n+-------------+-----+\n|Matei Zaharia|   CA|\n|  Reynold Xin|   CA|\n+-------------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+-----+\n|       Authos|State|\n+-------------+-----+\n|Matei Zaharia|   CA|\n|  Reynold Xin|   CA|\n+-------------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# DATAFRAME READER\n###################################################################################################################################\n\nfrom pyspark.sql.types import *\n# Programmatic way to define the schema (as we've seen above)\nfire_schema = StructType([StructField('CallNumber', IntegerType(), True),\n StructField('UnitID', StringType(), True),\n StructField('IncidentNumber', IntegerType(), True),\n StructField('CallType', StringType(), True), \n StructField('CallDate', StringType(), True), \n StructField('WatchDate', StringType(), True),\n StructField('CallFinalDisposition', StringType(), True),\n StructField('AvailableDtTm', StringType(), True),\n StructField('Address', StringType(), True), \n StructField('City', StringType(), True), \n StructField('Zipcode', IntegerType(), True), \n StructField('Battalion', StringType(), True), \n StructField('StationArea', StringType(), True), \n StructField('Box', StringType(), True), \n StructField('OriginalPriority', StringType(), True), \n StructField('Priority', StringType(), True), \n StructField('FinalPriority', IntegerType(), True), \n StructField('ALSUnit', BooleanType(), True), \n StructField('CallTypeGroup', StringType(), True),\n StructField('NumAlarms', IntegerType(), True),\n StructField('UnitType', StringType(), True),\n StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n StructField('FirePreventionDistrict', StringType(), True),\n StructField('SupervisorDistrict', StringType(), True),\n StructField('Neighborhood', StringType(), True),\n StructField('Location', StringType(), True),\n StructField('RowID', StringType(), True),\n StructField('Delay', FloatType(), True)])\n\n# Use the DFReader interface to read a CSV file\nsf_fire_file=\"/FileStore/tables/sf_fire_calls-1.csv\"\nfire_df=spark.read.csv(sf_fire_file,header=True,schema=fire_schema)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48db94e2-0579-4d03-8745-afaafb00a6a0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Saving a DF as a Parquet file or SQL table\n###################################################################################################\n\nparquet_table= #nombre_tabla\nfire_df.write.format(\"parquet\").saveAsTable(parquet_table)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70cf4f7a-5ac6-4447-b72c-cb777a050a7b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Projections and filters.\n# We can use these techniques to examine specific aspects of our SF Fire Department data set\n###################################################################################################################################\nfrom pyspark.sql.functions import col\nfew_fire_df=(fire_df\n            .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n            .where(col(\"CallType\") != \"Medical Incident\"))\nfew_fire_df.show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8496e387-e9a9-4e32-8794-f1667eddb719"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------+----------------------+--------------+\n|IncidentNumber|AvailableDtTm         |CallType      |\n+--------------+----------------------+--------------+\n|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n+--------------+----------------------+--------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------+----------------------+--------------+\n|IncidentNumber|AvailableDtTm         |CallType      |\n+--------------+----------------------+--------------+\n|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n+--------------+----------------------+--------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# What if we want to know how many distinct CallTypes were recorded as the causes of the fire calls? These simple and expressive queries do the job:\nfrom pyspark.sql.functions import *\n(fire_df\n .select(\"CallType\")\n.where(col(\"CallType\").isNotNull())\n.agg(countDistinct(\"CallType\").alias(\"DistinctCallTypes\"))\n.show())\n\n# We also can list the distinct call types in the data set using these queries:\n(fire_df\n .select(\"CallType\")\n.where(col(\"CallType\").isNotNull())\n.distinct()\n.show(10, False))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ff73ce0-2bc5-4e61-96de-55448abbc7b4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------+\n|DistinctCallTypes|\n+-----------------+\n|               30|\n+-----------------+\n\n+-----------------------------+\n|CallType                     |\n+-----------------------------+\n|Elevator / Escalator Rescue  |\n|Alarms                       |\n|Odor (Strange / Unknown)     |\n|Citizen Assist / Service Call|\n|Vehicle Fire                 |\n|Other                        |\n|Outside Fire                 |\n|Electrical Hazard            |\n|Structure Fire               |\n|Medical Incident             |\n+-----------------------------+\nonly showing top 10 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+\n|DistinctCallTypes|\n+-----------------+\n|               30|\n+-----------------+\n\n+-----------------------------+\n|CallType                     |\n+-----------------------------+\n|Elevator / Escalator Rescue  |\n|Alarms                       |\n|Odor (Strange / Unknown)     |\n|Citizen Assist / Service Call|\n|Vehicle Fire                 |\n|Other                        |\n|Outside Fire                 |\n|Electrical Hazard            |\n|Structure Fire               |\n|Medical Incident             |\n+-----------------------------+\nonly showing top 10 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# RENAMING, ADDING AND DROPPIN COLUMNS.\n###############################################################################################################################\nfrom pyspark.sql.functions import *\n\nnew_fire_df=fire_df.withColumnRenamed(\"Delay\",\"ResponseDelayedinMins\")\n(new_fire_df\n.select(\"ResponseDelayedinMins\")\n.where(col(\"ResponseDelayedinMins\")>5)\n.show(5,False))\n\n# Date formats\nfire_ts_df=(new_fire_df\n           .withColumn(\"IncidentDate\",to_timestamp(col(\"CallDate\"),\"MM/dd/yyyy\"))\n           .drop(\"CallDate\")\n           .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n           .drop(\"WatchDate\")\n           .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTM\"), \"MM/dd/yyyy hh:mm:ss a\"))\n           .drop(\"AvailableDtTM\"))\n(fire_ts_df\n.select(\"IncidentDate\",\"OnWatchDate\",\"AvailableDtTS\")\n.show(5,False))\n\n(fire_ts_df\n.select(year(\"IncidentDate\"))\n.distinct()\n.orderBy(year(\"IncidentDate\"))\n.show())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b28404c-56fd-4528-a8a0-41e2c1fd17c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------------------+\n|ResponseDelayedinMins|\n+---------------------+\n|5.35                 |\n|6.25                 |\n|5.2                  |\n|5.6                  |\n|7.25                 |\n+---------------------+\nonly showing top 5 rows\n\n+-------------------+-------------------+-------------------+\n|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n+-------------------+-------------------+-------------------+\n|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n+-------------------+-------------------+-------------------+\nonly showing top 5 rows\n\n+------------------+\n|year(IncidentDate)|\n+------------------+\n|              2000|\n|              2001|\n|              2002|\n|              2003|\n|              2004|\n|              2005|\n|              2006|\n|              2007|\n|              2008|\n|              2009|\n|              2010|\n|              2011|\n|              2012|\n|              2013|\n|              2014|\n|              2015|\n|              2016|\n|              2017|\n|              2018|\n+------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------------------+\n|ResponseDelayedinMins|\n+---------------------+\n|5.35                 |\n|6.25                 |\n|5.2                  |\n|5.6                  |\n|7.25                 |\n+---------------------+\nonly showing top 5 rows\n\n+-------------------+-------------------+-------------------+\n|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n+-------------------+-------------------+-------------------+\n|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n+-------------------+-------------------+-------------------+\nonly showing top 5 rows\n\n+------------------+\n|year(IncidentDate)|\n+------------------+\n|              2000|\n|              2001|\n|              2002|\n|              2003|\n|              2004|\n|              2005|\n|              2006|\n|              2007|\n|              2008|\n|              2009|\n|              2010|\n|              2011|\n|              2012|\n|              2013|\n|              2014|\n|              2015|\n|              2016|\n|              2017|\n|              2018|\n+------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# What were the most common types of fire calls?\n#######################################################################################################################################\n(fire_ts_df\n.select(\"CallType\")\n.where(col(\"CallType\").isNotNull())\n.groupBy(\"CallType\")\n.count()\n.orderBy(\"count\",ascending=False)\n.show(n=10,truncate=False))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23c99f57-5e91-44b9-b7cd-b9976ba4e70d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------------------------+------+\n|CallType                       |count |\n+-------------------------------+------+\n|Medical Incident               |113794|\n|Structure Fire                 |23319 |\n|Alarms                         |19406 |\n|Traffic Collision              |7013  |\n|Citizen Assist / Service Call  |2524  |\n|Other                          |2166  |\n|Outside Fire                   |2094  |\n|Vehicle Fire                   |854   |\n|Gas Leak (Natural and LP Gases)|764   |\n|Water Rescue                   |755   |\n+-------------------------------+------+\nonly showing top 10 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------------------------+------+\n|CallType                       |count |\n+-------------------------------+------+\n|Medical Incident               |113794|\n|Structure Fire                 |23319 |\n|Alarms                         |19406 |\n|Traffic Collision              |7013  |\n|Citizen Assist / Service Call  |2524  |\n|Other                          |2166  |\n|Outside Fire                   |2094  |\n|Vehicle Fire                   |854   |\n|Gas Leak (Natural and LP Gases)|764   |\n|Water Rescue                   |755   |\n+-------------------------------+------+\nonly showing top 10 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Other common DF operations\n############################################################################################################################\n\nimport pyspark.sql.functions as F\n\n(fire_ts_df\n.select(F.sum(\"NumAlarms\"), F.avg(\"ResponseDelayedinMins\"), F.min(\"ResponseDelayedinMins\"), F.max(\"ResponseDelayedinMins\"))\n.show())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9feeeae-e7a2-49b4-bbcf-0955caf52051"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------+--------------------------+--------------------------+--------------------------+\n|sum(NumAlarms)|avg(ResponseDelayedinMins)|min(ResponseDelayedinMins)|max(ResponseDelayedinMins)|\n+--------------+--------------------------+--------------------------+--------------------------+\n|        176170|         3.892364154521585|               0.016666668|                   1844.55|\n+--------------+--------------------------+--------------------------+--------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------+--------------------------+--------------------------+--------------------------+\n|sum(NumAlarms)|avg(ResponseDelayedinMins)|min(ResponseDelayedinMins)|max(ResponseDelayedinMins)|\n+--------------+--------------------------+--------------------------+--------------------------+\n|        176170|         3.892364154521585|               0.016666668|                   1844.55|\n+--------------+--------------------------+--------------------------+--------------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# END-TO-END DF EXAMPLE\n#########################################################################################################################3\n# 1. What were all the different types of fire calls in 2018?\n(fire_ts_df\n.select(\"CallType\")\n.where(year(\"IncidentDate\") == 2018)\n.distinct()\n.show(20,False))\n\n# 2. What months within the year 2018 saw the highest number of fire calls?\nfrom pyspark.sql.functions import col\n\n(fire_ts_df\n.select(\"NumAlarms\", month(\"IncidentDate\"),\"CallType\")\n.where(year(\"IncidentDate\")== 2018)\n .where(col(\"CallType\").like(\"%Fire%\"))\n.orderBy(\"NumAlarms\",ascending=False)\n.show(n=5,truncate=False))\n\n# 3. Which neighbourhood in San Francisco generated the most fire calls in 2018?\n(fire_ts_df\n.select(\"Neighborhood\", \"City\", \"NumAlarms\", year(\"IncidentDate\"), \"CallType\")\n.where(col(\"City\") == 'San Francisco')\n.where(year(\"IncidentDate\") == 2018)\n.where(col(\"CallType\").like(\"%Fire%\"))\n.orderBy(\"NumAlarms\",ascending=False)\n.show(n=3, truncate=False))\n\n# 4. Which neighborhood had the worst response times to fire calls in 2018?\n(fire_ts_df\n.select(\"Neighborhood\", \"City\", \"ResponseDelayedinMins\", year(\"IncidentDate\"), \"CallType\")\n.where(year(\"IncidentDate\") == 2018)\n.where(col(\"CallType\").like(\"%Fire%\"))\n.orderBy(\"ResponseDelayedinMins\",ascending=False)\n.show(n=3, truncate=False))\n\n# 5. Which week in the year 2018 had the most fire calls?\n\n(fire_ts_df\n.select(weekofyear(\"IncidentDate\"), \"NumAlarms\", \"City\", \"IncidentDate\", \"CallType\")\n.where(year(\"IncidentDate\") == 2018)\n.where(col(\"CallType\").like(\"%Fire%\"))\n.orderBy(\"NumAlarms\",ascending=False)\n.show(n=3,truncate=False))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"efdff6f8-2524-46c5-96b6-65296b280c89"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------------------------+\n|CallType                       |\n+-------------------------------+\n|Elevator / Escalator Rescue    |\n|Alarms                         |\n|Odor (Strange / Unknown)       |\n|Citizen Assist / Service Call  |\n|HazMat                         |\n|Vehicle Fire                   |\n|Other                          |\n|Outside Fire                   |\n|Traffic Collision              |\n|Assist Police                  |\n|Gas Leak (Natural and LP Gases)|\n|Water Rescue                   |\n|Electrical Hazard              |\n|Structure Fire                 |\n|Medical Incident               |\n|Fuel Spill                     |\n|Smoke Investigation (Outside)  |\n|Train / Rail Incident          |\n|Explosion                      |\n|Suspicious Package             |\n+-------------------------------+\n\n+---------+-------------------+--------------+\n|NumAlarms|month(IncidentDate)|CallType      |\n+---------+-------------------+--------------+\n|4        |3                  |Structure Fire|\n|3        |4                  |Structure Fire|\n|3        |10                 |Structure Fire|\n|3        |4                  |Structure Fire|\n|2        |6                  |Structure Fire|\n+---------+-------------------+--------------+\nonly showing top 5 rows\n\n+------------------------------+-------------+---------+------------------+--------------+\n|Neighborhood                  |City         |NumAlarms|year(IncidentDate)|CallType      |\n+------------------------------+-------------+---------+------------------+--------------+\n|Chinatown                     |San Francisco|4        |2018              |Structure Fire|\n|Inner Sunset                  |San Francisco|3        |2018              |Structure Fire|\n|Financial District/South Beach|San Francisco|3        |2018              |Structure Fire|\n+------------------------------+-------------+---------+------------------+--------------+\nonly showing top 3 rows\n\n+---------------------+-------------+---------------------+------------------+--------------+\n|Neighborhood         |City         |ResponseDelayedinMins|year(IncidentDate)|CallType      |\n+---------------------+-------------+---------------------+------------------+--------------+\n|Chinatown            |San Francisco|491.26666            |2018              |Structure Fire|\n|Pacific Heights      |San Francisco|129.01666            |2018              |Structure Fire|\n|Bayview Hunters Point|San Francisco|63.15                |2018              |Outside Fire  |\n+---------------------+-------------+---------------------+------------------+--------------+\nonly showing top 3 rows\n\n+------------------------+---------+-------------+-------------------+--------------+\n|weekofyear(IncidentDate)|NumAlarms|City         |IncidentDate       |CallType      |\n+------------------------+---------+-------------+-------------------+--------------+\n|11                      |4        |San Francisco|2018-03-17 00:00:00|Structure Fire|\n|16                      |3        |San Francisco|2018-04-21 00:00:00|Structure Fire|\n|43                      |3        |San Francisco|2018-10-22 00:00:00|Structure Fire|\n+------------------------+---------+-------------+-------------------+--------------+\nonly showing top 3 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------------------------+\n|CallType                       |\n+-------------------------------+\n|Elevator / Escalator Rescue    |\n|Alarms                         |\n|Odor (Strange / Unknown)       |\n|Citizen Assist / Service Call  |\n|HazMat                         |\n|Vehicle Fire                   |\n|Other                          |\n|Outside Fire                   |\n|Traffic Collision              |\n|Assist Police                  |\n|Gas Leak (Natural and LP Gases)|\n|Water Rescue                   |\n|Electrical Hazard              |\n|Structure Fire                 |\n|Medical Incident               |\n|Fuel Spill                     |\n|Smoke Investigation (Outside)  |\n|Train / Rail Incident          |\n|Explosion                      |\n|Suspicious Package             |\n+-------------------------------+\n\n+---------+-------------------+--------------+\n|NumAlarms|month(IncidentDate)|CallType      |\n+---------+-------------------+--------------+\n|4        |3                  |Structure Fire|\n|3        |4                  |Structure Fire|\n|3        |10                 |Structure Fire|\n|3        |4                  |Structure Fire|\n|2        |6                  |Structure Fire|\n+---------+-------------------+--------------+\nonly showing top 5 rows\n\n+------------------------------+-------------+---------+------------------+--------------+\n|Neighborhood                  |City         |NumAlarms|year(IncidentDate)|CallType      |\n+------------------------------+-------------+---------+------------------+--------------+\n|Chinatown                     |San Francisco|4        |2018              |Structure Fire|\n|Inner Sunset                  |San Francisco|3        |2018              |Structure Fire|\n|Financial District/South Beach|San Francisco|3        |2018              |Structure Fire|\n+------------------------------+-------------+---------+------------------+--------------+\nonly showing top 3 rows\n\n+---------------------+-------------+---------------------+------------------+--------------+\n|Neighborhood         |City         |ResponseDelayedinMins|year(IncidentDate)|CallType      |\n+---------------------+-------------+---------------------+------------------+--------------+\n|Chinatown            |San Francisco|491.26666            |2018              |Structure Fire|\n|Pacific Heights      |San Francisco|129.01666            |2018              |Structure Fire|\n|Bayview Hunters Point|San Francisco|63.15                |2018              |Outside Fire  |\n+---------------------+-------------+---------------------+------------------+--------------+\nonly showing top 3 rows\n\n+------------------------+---------+-------------+-------------------+--------------+\n|weekofyear(IncidentDate)|NumAlarms|City         |IncidentDate       |CallType      |\n+------------------------+---------+-------------+-------------------+--------------+\n|11                      |4        |San Francisco|2018-03-17 00:00:00|Structure Fire|\n|16                      |3        |San Francisco|2018-04-21 00:00:00|Structure Fire|\n|43                      |3        |San Francisco|2018-10-22 00:00:00|Structure Fire|\n+------------------------+---------+-------------+-------------------+--------------+\nonly showing top 3 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Typed Objects, Untyped Objects and Generic Rows\n#############################################################################################################\n\nfrom pyspark.sql import Row\nrow=Row(350,True, \"Learning Spark 2E\", None)\nrow[0]\nrow[1]\nrow[2]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c3f3a71-ae1e-41fb-a723-d8afeb7994d5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[73]: 'Learning Spark 2E'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[73]: 'Learning Spark 2E'"]}}],"execution_count":0},{"cell_type":"code","source":["# END TO END DATASET EXAMPLE\nco2ds=spark.read.json(\"/FileStore/tables/iot_devices.json\")\n# Detect failing devices with battery levels below a threshold\n(co2ds\n.select(\"device_name\",\"device_id\",\"battery_level\")\n.where(col(\"battery_level\")<5)\n.show(n=30))\n\n# Identify offending countries with high levels of CO2 emissions\n\n(co2ds\n.select(\"cn\",\"c02_level\")\n.where(col(\"c02_level\")> '1000')\n.show(n=20,truncate=False))\n\n# Compute the min and max values for temperature, battery level, CO2 and humidity\n(co2ds\n.select(min('temp'),max('temp'),min(\"battery_level\"),max(\"battery_level\"),min(\"c02_level\"),max(\"c02_level\"),min(\"humidity\"),max(\"humidity\")).show())\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd4f0e3f-519e-454e-88a3-dedf1cb7e979"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+---------+-------------+\n|         device_name|device_id|battery_level|\n+--------------------+---------+-------------+\n| device-mac-36TWSKiT|        3|            2|\n|therm-stick-5gimp...|        5|            4|\n|sensor-pad-6al7RT...|        6|            3|\n|meter-gauge-7GeDoanM|        7|            3|\n|sensor-pad-8xUD6p...|        8|            0|\n| device-mac-9GcjZ2pw|        9|            3|\n|meter-gauge-11dlM...|       11|            3|\n|sensor-pad-12Y2kIm0o|       12|            0|\n|sensor-pad-14QL93...|       14|            1|\n|sensor-pad-16aXmI...|       16|            4|\n|meter-gauge-17zb8...|       17|            0|\n|sensor-pad-18XULN9Xv|       18|            4|\n|therm-stick-25kK6...|       25|            4|\n|sensor-pad-28Tsud...|       28|            3|\n|device-mac-33B94G...|       33|            3|\n|sensor-pad-36VQv8...|       36|            1|\n|device-mac-39iklY...|       39|            2|\n| sensor-pad-40NjeMqS|       40|            2|\n|meter-gauge-43RYo...|       43|            2|\n| sensor-pad-448DeWGL|       44|            0|\n|device-mac-45fN2C...|       45|            4|\n|meter-gauge-47WsF9s8|       47|            3|\n|meter-gauge-49Yes...|       49|            4|\n| sensor-pad-52eFObBC|       52|            2|\n| sensor-pad-58HgZVw0|       58|            4|\n|sensor-pad-62fH8o...|       62|            3|\n|device-mac-63GL4x...|       63|            4|\n|  sensor-pad-64djcIn|       64|            3|\n|therm-stick-65V4W...|       65|            3|\n|device-mac-69kjyS...|       69|            4|\n+--------------------+---------+-------------+\nonly showing top 30 rows\n\n+-------------+---------+\n|cn           |c02_level|\n+-------------+---------+\n|Norway       |1473     |\n|Italy        |1556     |\n|United States|1080     |\n|United States|1210     |\n|China        |1129     |\n|Japan        |1536     |\n|United States|1470     |\n|Italy        |1544     |\n|United States|1260     |\n|India        |1007     |\n|Norway       |1346     |\n|United States|1259     |\n|United States|1425     |\n|United States|1466     |\n|China        |1096     |\n|United States|1531     |\n|United States|1155     |\n|Japan        |1522     |\n|India        |1245     |\n|Canada       |1511     |\n+-------------+---------+\nonly showing top 20 rows\n\n+---------+---------+------------------+------------------+--------------+--------------+-------------+-------------+\n|min(temp)|max(temp)|min(battery_level)|max(battery_level)|min(c02_level)|max(c02_level)|min(humidity)|max(humidity)|\n+---------+---------+------------------+------------------+--------------+--------------+-------------+-------------+\n|       10|       34|                 0|                 9|           800|          1599|           25|           99|\n+---------+---------+------------------+------------------+--------------+--------------+-------------+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+---------+-------------+\n|         device_name|device_id|battery_level|\n+--------------------+---------+-------------+\n| device-mac-36TWSKiT|        3|            2|\n|therm-stick-5gimp...|        5|            4|\n|sensor-pad-6al7RT...|        6|            3|\n|meter-gauge-7GeDoanM|        7|            3|\n|sensor-pad-8xUD6p...|        8|            0|\n| device-mac-9GcjZ2pw|        9|            3|\n|meter-gauge-11dlM...|       11|            3|\n|sensor-pad-12Y2kIm0o|       12|            0|\n|sensor-pad-14QL93...|       14|            1|\n|sensor-pad-16aXmI...|       16|            4|\n|meter-gauge-17zb8...|       17|            0|\n|sensor-pad-18XULN9Xv|       18|            4|\n|therm-stick-25kK6...|       25|            4|\n|sensor-pad-28Tsud...|       28|            3|\n|device-mac-33B94G...|       33|            3|\n|sensor-pad-36VQv8...|       36|            1|\n|device-mac-39iklY...|       39|            2|\n| sensor-pad-40NjeMqS|       40|            2|\n|meter-gauge-43RYo...|       43|            2|\n| sensor-pad-448DeWGL|       44|            0|\n|device-mac-45fN2C...|       45|            4|\n|meter-gauge-47WsF9s8|       47|            3|\n|meter-gauge-49Yes...|       49|            4|\n| sensor-pad-52eFObBC|       52|            2|\n| sensor-pad-58HgZVw0|       58|            4|\n|sensor-pad-62fH8o...|       62|            3|\n|device-mac-63GL4x...|       63|            4|\n|  sensor-pad-64djcIn|       64|            3|\n|therm-stick-65V4W...|       65|            3|\n|device-mac-69kjyS...|       69|            4|\n+--------------------+---------+-------------+\nonly showing top 30 rows\n\n+-------------+---------+\n|cn           |c02_level|\n+-------------+---------+\n|Norway       |1473     |\n|Italy        |1556     |\n|United States|1080     |\n|United States|1210     |\n|China        |1129     |\n|Japan        |1536     |\n|United States|1470     |\n|Italy        |1544     |\n|United States|1260     |\n|India        |1007     |\n|Norway       |1346     |\n|United States|1259     |\n|United States|1425     |\n|United States|1466     |\n|China        |1096     |\n|United States|1531     |\n|United States|1155     |\n|Japan        |1522     |\n|India        |1245     |\n|Canada       |1511     |\n+-------------+---------+\nonly showing top 20 rows\n\n+---------+---------+------------------+------------------+--------------+--------------+-------------+-------------+\n|min(temp)|max(temp)|min(battery_level)|max(battery_level)|min(c02_level)|max(c02_level)|min(humidity)|max(humidity)|\n+---------+---------+------------------+------------------+--------------+--------------+-------------+-------------+\n|       10|       34|                 0|                 9|           800|          1599|           25|           99|\n+---------+---------+------------------+------------------+--------------+--------------+-------------+-------------+\n\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-149898280422770>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[0;31m# Sort and group by temperature, c02, humidity and country\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 21\u001B[0;31m (co2ds\n\u001B[0m\u001B[1;32m     22\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mgroupBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"temp\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"c02_level\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"humidity\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"cn\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/group.py\u001B[0m in \u001B[0;36m_api\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m     39\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_api\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m         \u001B[0mname\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 41\u001B[0;31m         \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jgd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_to_seq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql_ctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     42\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql_ctx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m     \u001B[0m_api\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Column 'temp' does not exist. Did you mean one of the following? [];\n'Aggregate ['temp, 'c02_level, 'humidity, 'cn], ['temp, 'c02_level, 'humidity, 'cn]\n+- Project\n   +- Relation [battery_level#5976L,c02_level#5977L,cca2#5978,cca3#5979,cn#5980,device_id#5981L,device_name#5982,humidity#5983L,ip#5984,latitude#5985,lcd#5986,longitude#5987,scale#5988,temp#5989L,timestamp#5990L] json\n","errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: Column 'temp' does not exist. Did you mean one of the following? [];\n'Aggregate ['temp, 'c02_level, 'humidity, 'cn], ['temp, 'c02_level, 'humidity, 'cn]\n+- Project\n   +- Relation [battery_level#5976L,c02_level#5977L,cca2#5978,cca3#5979,cn#5980,device_id#5981L,device_name#5982,humidity#5983L,ip#5984,latitude#5985,lcd#5986,longitude#5987,scale#5988,temp#5989L,timestamp#5990L] json\n","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-149898280422770>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[0;31m# Sort and group by temperature, c02, humidity and country\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 21\u001B[0;31m (co2ds\n\u001B[0m\u001B[1;32m     22\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mgroupBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"temp\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"c02_level\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"humidity\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"cn\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/group.py\u001B[0m in \u001B[0;36m_api\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m     39\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_api\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m         \u001B[0mname\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 41\u001B[0;31m         \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jgd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_to_seq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql_ctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     42\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql_ctx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m     \u001B[0m_api\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Column 'temp' does not exist. Did you mean one of the following? [];\n'Aggregate ['temp, 'c02_level, 'humidity, 'cn], ['temp, 'c02_level, 'humidity, 'cn]\n+- Project\n   +- Relation [battery_level#5976L,c02_level#5977L,cca2#5978,cca3#5979,cn#5980,device_id#5981L,device_name#5982,humidity#5983L,ip#5984,latitude#5985,lcd#5986,longitude#5987,scale#5988,temp#5989L,timestamp#5990L] json\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"CAP3_Python","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":149898280422736}},"nbformat":4,"nbformat_minor":0}
