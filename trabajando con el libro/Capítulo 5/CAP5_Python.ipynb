{"cells":[{"cell_type":"markdown","source":["# 1. Funciones definidas por el usuario #\n\nUser defined functions: UDFs\n\n### Spark SQL UDFs ###"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8945643-35af-4005-a422-dae0b55c29d4"}}},{"cell_type":"code","source":["from pyspark.sql.types import LongType\n\n# Creamos la función cúbica\ndef cubed(s):\n    return s * s * s\n# Registramos la UDF\nspark.udf.register(\"cubed\",cubed,LongType())\n#Generamos una vista temporal\nspark.range(1,9).createOrReplaceTempView(\"udf_test\")\n\nspark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de8884be-c01c-4e39-b2d2-42ac487576d7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+--------+\n| id|id_cubed|\n+---+--------+\n|  1|       1|\n|  2|       8|\n|  3|      27|\n|  4|      64|\n|  5|     125|\n|  6|     216|\n|  7|     343|\n|  8|     512|\n+---+--------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+--------+\n| id|id_cubed|\n+---+--------+\n|  1|       1|\n|  2|       8|\n|  3|      27|\n|  4|      64|\n|  5|     125|\n|  6|     216|\n|  7|     343|\n|  8|     512|\n+---+--------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Remarcamos que Spark SQL no garantiza el order de evaluación de las subexpresiones. Así, para hacer bien el *null checking*, se recomienda:\n1. Hacer la propia UDF *null-aware* y que haga el null-checking dentro de la UDF.\n2. Usar IF o CASE WHEN para hacer el null check e invocar la UDF en una rama condicional\n\n#### Acelerando y distribuyendo Pyspark UDFs con Pandas UDFs ####\n\nPanda UDF: UDF vectorizado.\n\nEl siguiente ejemplo es de un Pand UDF escalar para Spark 3.0.:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a5ba920-ed8b-43b3-bfac-ccab014295b4"}}},{"cell_type":"code","source":["import pandas as pd\nfrom pyspark.sql.functions import col, pandas_udf\nfrom pyspark.sql.types import LongType\n\ndef cubed(a: pd.Series) -> pd.Series:\n    return a * a * a\n\n# Create the pandas UDF for the cubed function\ncubed_udf=pandas_udf(cubed,returnType=LongType())\n\n# Este código declara una función llamada cubed() que performa una operación *cubed*. \n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1482b6f7-9ae4-4fab-9c04-c9a6dfbf3e47"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Empecemos con una simple Panda Series y luego aplicar la función local cubed() para el cálculo cube."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d61cea61-6e7c-40d2-abe4-a839defc6e02"}}},{"cell_type":"code","source":["# Create a Pandas Series\nx=pd.Series([1,2,3])\n\n# La función para una pandas_udf ejecutada con datos locales Pandas\nprint(cubed(x))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6601c5e9-82bb-433e-8df5-c90c51dea424"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"0     1\n1     8\n2    27\ndtype: int64\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["0     1\n1     8\n2    27\ndtype: int64\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Cambiemos a un Spark DataFrame. Podemos ejecutar esta función como una Spark UDF vectorizada como sigue:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"351bd250-21ee-440c-9a2e-eeb333831239"}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark = (SparkSession\n        .builder\n        .appName(\"AuthorsAges\")\n        .getOrCreate())\n\n# Creamos un Spark DF\n\ndf=spark.range(1,4)\n\n# Ejecutamos la función como una Spark UDF vectorizada\n\ndf.select(\"id\",cubed_udf(col(\"id\"))).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c5a5f62-3539-4d94-8b1a-2578ba2456df"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+---------+\n| id|cubed(id)|\n+---+---------+\n|  1|        1|\n|  2|        8|\n|  3|       27|\n+---+---------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---------+\n| id|cubed(id)|\n+---+---------+\n|  1|        1|\n|  2|        8|\n|  3|       27|\n+---+---------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Opuestamente a una función local, usar una UDF vectorizada resultará en la ejecución de Spark jobs; la función local previa es una función Pandas ejecutada sólo en el driver de Spark."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54cf0752-db0a-46b0-ad8e-1a40c4326239"}}},{"cell_type":"markdown","source":["# 2. Funciones high-order en DataFrames y Spark SQL #\n\nHay dos soluciones típicas para manipular tipos de datos complejos:\n- Explotar la estructura anidada en filas individuales, aplicando alguna función y luego recreando la estructura.\n- Creando una función user-defined\n\n### Opción 1: Explotar y recoger ###\nEn esta declaración SQL anidada, primero explotamos los values, creando así una nueva fila (con el id) para cada elemento (value) en values:\n\n```\nEn SQL:\nSELECT id, collect_list(value+1) AS values\nFROM (SELECT id, EXPLODE(values) AS value\n        FROM table) x\nGROUP BY id\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93724d68-e684-403c-a3a0-af783cb7ef0a"}}},{"cell_type":"code","source":["# Create an array dataset\narrayData = [[1, (1, 2, 3)], [2, (2, 3, 4)], [3, (3, 4, 5)]]\n\n# Create schema\nfrom pyspark.sql.types import *\narraySchema = (StructType([\n      StructField(\"id\", IntegerType(), True), \n      StructField(\"values\", ArrayType(IntegerType()), True)\n      ]))\n\n# Create DataFrame\ndf = spark.createDataFrame(spark.sparkContext.parallelize(arrayData), arraySchema)\ndf.createOrReplaceTempView(\"table\")\ndf.printSchema()\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1aa1fd08-341e-444e-875f-4437c9fb57ba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- id: integer (nullable = true)\n |-- values: array (nullable = true)\n |    |-- element: integer (containsNull = true)\n\n+---+---------+\n| id|   values|\n+---+---------+\n|  1|[1, 2, 3]|\n|  2|[2, 3, 4]|\n|  3|[3, 4, 5]|\n+---+---------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- id: integer (nullable = true)\n |-- values: array (nullable = true)\n |    |-- element: integer (containsNull = true)\n\n+---+---------+\n| id|   values|\n+---+---------+\n|  1|[1, 2, 3]|\n|  2|[2, 3, 4]|\n|  3|[3, 4, 5]|\n+---+---------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"\"\"\nSELECT id, collect_list(value+1) AS values \n    FROM (SELECT id, EXPLODE(values) AS value\n        FROM table) x \n    GROUP BY id\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e17fd45e-ff43-446c-ae85-a452366110b0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+---------+\n| id|   values|\n+---+---------+\n|  1|[2, 3, 4]|\n|  2|[3, 4, 5]|\n|  3|[4, 5, 6]|\n+---+---------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---------+\n| id|   values|\n+---+---------+\n|  1|[2, 3, 4]|\n|  2|[3, 4, 5]|\n|  3|[4, 5, 6]|\n+---+---------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Opción 2: Funciones definidas por usuario ###\n\nPara ejecutar la misma tarea, podemos crear una UDF que use map() para iterar a través de cada elemento (value) y ejecutar la operación suma.\nMientras que esto es mejor que usar explode() y collect_list(), los procesos de serialización y deserialización puede ser costoso. También recalcar que collect_list() puede causa que los ejecutores pasen por problemas out-of-memory para datasets grandes, mientras que usando UDFs podríamos aliviar esas cuestiones."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0154aedf-56a1-4b7e-b3c7-e4e59d1e2571"}}},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType\nfrom pyspark.sql.types import ArrayType\n\n# Create UDF\ndef addOne(values):\n  return [value + 1 for value in values]\n\n# Register UDF\nspark.udf.register(\"plusOneInt\", addOne, ArrayType(IntegerType())) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e92afc7-0be8-4655-a362-4ed4cd348293"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[12]: <function __main__.addOne(values)>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[12]: <function __main__.addOne(values)>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"SELECT id, plusOneInt(values) AS values FROM table\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"218a07f3-8c4a-4a4e-a5f4-c4eba18d1ef5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+---------+\n| id|   values|\n+---+---------+\n|  1|[2, 3, 4]|\n|  2|[3, 4, 5]|\n|  3|[4, 5, 6]|\n+---+---------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---------+\n| id|   values|\n+---+---------+\n|  1|[2, 3, 4]|\n|  2|[3, 4, 5]|\n|  3|[4, 5, 6]|\n+---+---------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Funciones de orden alto ###\n\nHay funciones de orden alto que toma funciones lambda anónimas como argumentos. Un ejemplo de función de orden alto es la siguiente:\n```\nspark.sql(\"transform(values,value->lambda expression)\")\n```\ndonde transform() coge el array values y una función anónima (lambda expression) como entrada. La función crea un nuevo array aplicando la función anónima a cada elemento, y luego asignando el resultado al array de salida.\n\nCreamos a continuación un dataset para ver algunos ejemplos."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8dde5660-f2f1-4969-90b7-f7a56104ecdc"}}},{"cell_type":"code","source":["from pyspark.sql.types import *\nschema = StructType([StructField(\"celsius\", ArrayType(IntegerType()))])\n\nt_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\nt_c = spark.createDataFrame(t_list, schema)\nt_c.createOrReplaceTempView(\"tC\")\n\n# Show the DataFrame\nt_c.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46c91ed5-1126-452d-8d08-2d8c5165e7be"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+\n|             celsius|\n+--------------------+\n|[35, 36, 32, 30, ...|\n|[31, 32, 34, 55, 56]|\n+--------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+\n|             celsius|\n+--------------------+\n|[35, 36, 32, 30, ...|\n|[31, 32, 34, 55, 56]|\n+--------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Transform() ####\n\nProduce un array aplicando la función a cada elemento del array de entrada (similar a map())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00943d3b-3c7e-4edf-9c08-c204e3305bbe"}}},{"cell_type":"code","source":["# Calcular Fahrenheit a Celsius para un array de temperaturas\n\nspark.sql(\"\"\"\nSELECT celsius, transform(celsius, t->((t*9) div 5) +32) as fahrenheit\n    FROM tC\n\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44393d64-4d74-4226-b82b-b2b26b0f064c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+--------------------+\n|             celsius|          fahrenheit|\n+--------------------+--------------------+\n|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n+--------------------+--------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+--------------------+\n|             celsius|          fahrenheit|\n+--------------------+--------------------+\n|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n+--------------------+--------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Filter() ####\n\nProduce un array consistente en sólo elementos del array de entrada para los cuales la función booleana es true:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe48ddf0-d35f-489d-92aa-74a577777901"}}},{"cell_type":"code","source":["# Filtramos las temperaturas mayores de 38ºC\n\nspark.sql(\"\"\"\nSELECT celsius,\n    filter(celsius,t->t>38) as high\n    FROM tC\n\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83894daa-875d-417c-b2f2-e511bba76f6d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+--------+\n|             celsius|    high|\n+--------------------+--------+\n|[35, 36, 32, 30, ...|[40, 42]|\n|[31, 32, 34, 55, 56]|[55, 56]|\n+--------------------+--------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+--------+\n|             celsius|    high|\n+--------------------+--------+\n|[35, 36, 32, 30, ...|[40, 42]|\n|[31, 32, 34, 55, 56]|[55, 56]|\n+--------------------+--------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Exists() ####\n\nDevuelve true si la función booleana se mantiene para algún elemento del array de entrada:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9b2359e-f9bb-4737-8870-ac08eaa4b1d1"}}},{"cell_type":"code","source":["# Hay alguna temperatura de 38ºC?\n\nspark.sql(\"\"\"\nSELECT celsius,\n       exists(celsius,t->t=38) as threshold\nFROM tC\n\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"825a5c2a-501c-4b11-af4c-4a711ed022ea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+---------+\n|             celsius|threshold|\n+--------------------+---------+\n|[35, 36, 32, 30, ...|     true|\n|[31, 32, 34, 55, 56]|    false|\n+--------------------+---------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+---------+\n|             celsius|threshold|\n+--------------------+---------+\n|[35, 36, 32, 30, ...|     true|\n|[31, 32, 34, 55, 56]|    false|\n+--------------------+---------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Reduce() ####\n\nReduce los elementos del array a un valor único fusionando los elementos a un buffer B usando function<B,T,B> y aplicando una función finalizadora function<B,R> en el buffer final:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1be2d1db-7cb1-4642-98ba-209dda1f1c37"}}},{"cell_type":"code","source":["# Calcular la temperatura media y convertir a F\n\nspark.sql(\"\"\"\nSELECT celsius,\n       reduce(\n         celsius, / \n         0,\n         (t,acc)->t+acc,\n         acc-> (acc div size(celsius)*9 div 5) +32\n         ) as avgFahrenheit\nFROM tC\n\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c8c70ab-3e6a-499e-b216-409075400acd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+-------------+\n|             celsius|avgFahrenheit|\n+--------------------+-------------+\n|[35, 36, 32, 30, ...|           96|\n|[31, 32, 34, 55, 56]|          105|\n+--------------------+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+-------------+\n|             celsius|avgFahrenheit|\n+--------------------+-------------+\n|[35, 36, 32, 30, ...|           96|\n|[31, 32, 34, 55, 56]|          105|\n+--------------------+-------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["# 3. Operaciones comunes de DataFrames y SparkSQL #\n\nVamos a preparar datos. Así:\n1. Importamos dos archivos y creamos dos DataFrames, uno para información de aeropuerto y otro para retrasos en los vuelos de US.\n2. Usando expr(), convertimos las columnas delay y distance de STRING a INT.\n3. Creamos una tabla más pequeña, foo, en la que podemos centrarnos en nuestros ejemplos demo; contiene la información sólo de 3 vuelos desde SEA a SFO."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eebfa0ab-22c3-4a18-945f-1cfd39046507"}}},{"cell_type":"code","source":["from pyspark.sql.functions import expr\ntripdelaysFilePath=\"/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\nairportsnaFilePath=\"/databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt\"\n\nairportsna=(spark.read.format(\"csv\").options(header=\"true\",inferSchema=\"true\",sep=\"\\t\").load(airportsnaFilePath))\nairportsna.createOrReplaceTempView(\"airports_na\")\n\ndepartureDelays=(spark.read.format(\"csv\").options(header=\"true\").load(tripdelaysFilePath))\ndepartureDelays=(departureDelays.withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\")).withColumn(\"distance\",expr(\"CAST(distance as INT) as distance\")))\ndepartureDelays.createOrReplaceTempView(\"departureDelays\")\n\nfoo=(departureDelays.filter(expr(\"\"\"origin == 'SEA' and destination == 'SFO' and date like '01010%' and delay>0\"\"\")))\nfoo.createOrReplaceTempView(\"foo\")\n\nspark.sql(\"SELECT * FROM foo\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d924e096-40cf-4578-9032-fe5a51813dad"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+-----+--------+------+-----------+\n|    date|delay|distance|origin|destination|\n+--------+-----+--------+------+-----------+\n|01010710|   31|     590|   SEA|        SFO|\n|01010955|  104|     590|   SEA|        SFO|\n|01010730|    5|     590|   SEA|        SFO|\n+--------+-----+--------+------+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+-----+--------+------+-----------+\n|    date|delay|distance|origin|destination|\n+--------+-----+--------+------+-----------+\n|01010710|   31|     590|   SEA|        SFO|\n|01010955|  104|     590|   SEA|        SFO|\n|01010730|    5|     590|   SEA|        SFO|\n+--------+-----+--------+------+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Uniones ###\n\nUnamos dos tablas. El dataframe 'bar' es la unión de foo con delays."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1724259f-4289-4e98-a3e5-c8c5e75b71ef"}}},{"cell_type":"code","source":["bar=departureDelays.union(foo)\nbar.createOrReplaceTempView(\"bar\")\n\nbar.filter(expr(\"\"\"origin=='SEA' AND destination=='SFO' AND date like '01010%' and delay>0\"\"\")).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a26cb4d-07c5-452d-8956-8ac3270d52b1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+-----+--------+------+-----------+\n|    date|delay|distance|origin|destination|\n+--------+-----+--------+------+-----------+\n|01010710|   31|     590|   SEA|        SFO|\n|01010955|  104|     590|   SEA|        SFO|\n|01010730|    5|     590|   SEA|        SFO|\n|01010710|   31|     590|   SEA|        SFO|\n|01010955|  104|     590|   SEA|        SFO|\n|01010730|    5|     590|   SEA|        SFO|\n+--------+-----+--------+------+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+-----+--------+------+-----------+\n|    date|delay|distance|origin|destination|\n+--------+-----+--------+------+-----------+\n|01010710|   31|     590|   SEA|        SFO|\n|01010955|  104|     590|   SEA|        SFO|\n|01010730|    5|     590|   SEA|        SFO|\n|01010710|   31|     590|   SEA|        SFO|\n|01010955|  104|     590|   SEA|        SFO|\n|01010730|    5|     590|   SEA|        SFO|\n+--------+-----+--------+------+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Joins ###\n\nPor defecto, Spark SQL hace el join como un inner join, con las opciones inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi y left_anti.\nEl siguiente ejemplo ejecuta una inner join entre los dataframes airportsna y foo"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e8823ce-4268-40a9-8ad5-483814f69698"}}},{"cell_type":"code","source":["# Join foo con airport info.\n\nfoo.join(\n    airportsna,\n    airportsna.IATA == foo.origin\n).select(\"City\",\"State\",\"date\",\"delay\",\"distance\",\"destination\").show()\n\n# En SQL:\nspark.sql(\"\"\"\nSELECT a.City,a.State,f.date,f.delay,f.distance,f.destination\nFROM foo f\nJOIN airports_na a \n    ON a.IATA=f.origin\n\"\"\").show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8e42a2f-b65c-454a-b66b-5632d7522862"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+-----+--------+-----+--------+-----------+\n|   City|State|    date|delay|distance|destination|\n+-------+-----+--------+-----+--------+-----------+\n|Seattle|   WA|01010710|   31|     590|        SFO|\n|Seattle|   WA|01010955|  104|     590|        SFO|\n|Seattle|   WA|01010730|    5|     590|        SFO|\n+-------+-----+--------+-----+--------+-----------+\n\n+-------+-----+--------+-----+--------+-----------+\n|   City|State|    date|delay|distance|destination|\n+-------+-----+--------+-----+--------+-----------+\n|Seattle|   WA|01010710|   31|     590|        SFO|\n|Seattle|   WA|01010955|  104|     590|        SFO|\n|Seattle|   WA|01010730|    5|     590|        SFO|\n+-------+-----+--------+-----+--------+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+-----+--------+-----+--------+-----------+\n|   City|State|    date|delay|distance|destination|\n+-------+-----+--------+-----+--------+-----------+\n|Seattle|   WA|01010710|   31|     590|        SFO|\n|Seattle|   WA|01010955|  104|     590|        SFO|\n|Seattle|   WA|01010730|    5|     590|        SFO|\n+-------+-----+--------+-----+--------+-----------+\n\n+-------+-----+--------+-----+--------+-----------+\n|   City|State|    date|delay|distance|destination|\n+-------+-----+--------+-----+--------+-----------+\n|Seattle|   WA|01010710|   31|     590|        SFO|\n|Seattle|   WA|01010955|  104|     590|        SFO|\n|Seattle|   WA|01010730|    5|     590|        SFO|\n+-------+-----+--------+-----+--------+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Windowing ###\n\nUna función ventana usa valores de las filas en una ventana para devolver un conjunto de valores, tipicamente en forma de otra fila. Hacen posible operar en un grupo de filas mientras que aún está devolviendo un único valor para cada fila de entrada.\nVeremos en esta sección como usar dense_rank().\n\nEmpecemos con una review de TotalDelays (calculado por sum(Delay)) experimentado por vuelos de origen en SEA, SFO y JFK y yendo a un conjunto específico de destinos, como notamos en la siguiente consulta:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0cd0bba5-49a7-464e-bb3b-46a394534d52"}}},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS departureDelaysWindow\")\n\nspark.sql(\"\"\"\nCREATE TABLE departureDelaysWindow AS\nSELECT origin, destination, SUM(delay) AS TotalDelays\n    FROM departureDelays\n    WHERE origin IN ('SEA','SFO','JFK')\n        AND DESTINATION IN ('SEA','SFO','JFK','DEN','ORD','LAX','ATL')\n    GROUP BY origin, destination;\n\"\"\")\n\nspark.sql(\"SELECT * FROM departureDelaysWindow\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5863150-573e-4fff-b9de-ac0aed7f93df"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+-----------+-----------+\n|origin|destination|TotalDelays|\n+------+-----------+-----------+\n|   JFK|        ORD|       5608|\n|   JFK|        SFO|      35619|\n|   JFK|        DEN|       4315|\n|   JFK|        ATL|      12141|\n|   JFK|        SEA|       7856|\n|   JFK|        LAX|      35755|\n|   SEA|        LAX|       9359|\n|   SFO|        ORD|      27412|\n|   SFO|        DEN|      18688|\n|   SFO|        SEA|      17080|\n|   SEA|        SFO|      22293|\n|   SFO|        ATL|       5091|\n|   SEA|        DEN|      13645|\n|   SEA|        ATL|       4535|\n|   SEA|        ORD|      10041|\n|   SFO|        JFK|      24100|\n|   SFO|        LAX|      40798|\n|   SEA|        JFK|       4667|\n+------+-----------+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+-----------+-----------+\n|origin|destination|TotalDelays|\n+------+-----------+-----------+\n|   JFK|        ORD|       5608|\n|   JFK|        SFO|      35619|\n|   JFK|        DEN|       4315|\n|   JFK|        ATL|      12141|\n|   JFK|        SEA|       7856|\n|   JFK|        LAX|      35755|\n|   SEA|        LAX|       9359|\n|   SFO|        ORD|      27412|\n|   SFO|        DEN|      18688|\n|   SFO|        SEA|      17080|\n|   SEA|        SFO|      22293|\n|   SFO|        ATL|       5091|\n|   SEA|        DEN|      13645|\n|   SEA|        ATL|       4535|\n|   SEA|        ORD|      10041|\n|   SFO|        JFK|      24100|\n|   SFO|        LAX|      40798|\n|   SEA|        JFK|       4667|\n+------+-----------+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Si quisiéramos ver para cada uno de los aeropuertos de origen los 3 destinos que experimentan más retrasos:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"857587d9-36f5-41f0-8a33-3d48ac4998e4"}}},{"cell_type":"code","source":["spark.sql(\"\"\"\nSELECT origin,destination, TotalDelays, rank\n    FROM (\n        SELECT origin, destination,TotalDelays,dense_rank()\n        OVER (PARTITION BY origin ORDER BY TotalDelays DESC) as rank\n        FROM departureDelaysWindow\n) t\nWHERE rank <=3\n\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a7f59d9-f1ed-4ed2-bd42-fc49de0e2be0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+-----------+-----------+----+\n|origin|destination|TotalDelays|rank|\n+------+-----------+-----------+----+\n|   JFK|        LAX|      35755|   1|\n|   JFK|        SFO|      35619|   2|\n|   JFK|        ATL|      12141|   3|\n|   SEA|        SFO|      22293|   1|\n|   SEA|        DEN|      13645|   2|\n|   SEA|        ORD|      10041|   3|\n|   SFO|        LAX|      40798|   1|\n|   SFO|        ORD|      27412|   2|\n|   SFO|        JFK|      24100|   3|\n+------+-----------+-----------+----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+-----------+-----------+----+\n|origin|destination|TotalDelays|rank|\n+------+-----------+-----------+----+\n|   JFK|        LAX|      35755|   1|\n|   JFK|        SFO|      35619|   2|\n|   JFK|        ATL|      12141|   3|\n|   SEA|        SFO|      22293|   1|\n|   SEA|        DEN|      13645|   2|\n|   SEA|        ORD|      10041|   3|\n|   SFO|        LAX|      40798|   1|\n|   SFO|        ORD|      27412|   2|\n|   SFO|        JFK|      24100|   3|\n+------+-----------+-----------+----+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Modificaciones ###\n\nMientras que los DataFrames por sí mismos son inmutables, podemos modificarlos mediante operaciones que creen nuevos y diferentes DataFrames, con distintas columnas, por ejemplo.\n\n#### Añadir nuevas columnas ####\n\nUsamos el comando withColumn()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72cf0c9e-2b6e-4691-a500-c4dbb39947ac"}}},{"cell_type":"code","source":["from pyspark.sql.functions import expr\nfoo2=(foo.withColumn(\n          \"status\",\n           expr(\"CASE WHEN delay<=10 THEN 'On-time' ELSE 'Delayed' END\")\n))\nfoo2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2fa2538-9b81-4f55-9e61-4326eacb0390"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+-----+--------+------+-----------+-------+\n|    date|delay|distance|origin|destination| status|\n+--------+-----+--------+------+-----------+-------+\n|01010710|   31|     590|   SEA|        SFO|Delayed|\n|01010955|  104|     590|   SEA|        SFO|Delayed|\n|01010730|    5|     590|   SEA|        SFO|On-time|\n+--------+-----+--------+------+-----------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+-----+--------+------+-----------+-------+\n|    date|delay|distance|origin|destination| status|\n+--------+-----+--------+------+-----------+-------+\n|01010710|   31|     590|   SEA|        SFO|Delayed|\n|01010955|  104|     590|   SEA|        SFO|Delayed|\n|01010730|    5|     590|   SEA|        SFO|On-time|\n+--------+-----+--------+------+-----------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Dropeando columnas ####\n\nUsamos el método drop()."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03d76f5a-f550-4fba-ac0a-2c7f0a600271"}}},{"cell_type":"code","source":["foo3=foo2.drop(\"delay\")\nfoo3.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4a5a690-55e8-4128-a71b-aeb630ddf986"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+--------+------+-----------+-------+\n|    date|distance|origin|destination| status|\n+--------+--------+------+-----------+-------+\n|01010710|     590|   SEA|        SFO|Delayed|\n|01010955|     590|   SEA|        SFO|Delayed|\n|01010730|     590|   SEA|        SFO|On-time|\n+--------+--------+------+-----------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+--------+------+-----------+-------+\n|    date|distance|origin|destination| status|\n+--------+--------+------+-----------+-------+\n|01010710|     590|   SEA|        SFO|Delayed|\n|01010955|     590|   SEA|        SFO|Delayed|\n|01010730|     590|   SEA|        SFO|On-time|\n+--------+--------+------+-----------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Renombrando columnas ####\n\nUsando rename()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53b4188d-65ae-4a54-a6d9-9c8824f9c08a"}}},{"cell_type":"code","source":["foo4=foo3.withColumnRenamed(\"status\",\"flight_status\")\nfoo4.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"987ba926-1bba-4ac5-8476-32f266b51e2c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+--------+------+-----------+-------------+\n|    date|distance|origin|destination|flight_status|\n+--------+--------+------+-----------+-------------+\n|01010710|     590|   SEA|        SFO|      Delayed|\n|01010955|     590|   SEA|        SFO|      Delayed|\n|01010730|     590|   SEA|        SFO|      On-time|\n+--------+--------+------+-----------+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+--------+------+-----------+-------------+\n|    date|distance|origin|destination|flight_status|\n+--------+--------+------+-----------+-------------+\n|01010710|     590|   SEA|        SFO|      Delayed|\n|01010955|     590|   SEA|        SFO|      Delayed|\n|01010730|     590|   SEA|        SFO|      On-time|\n+--------+--------+------+-----------+-------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Pivotando ####\n\nCuando trabajamos con datos, a veces necesitaremos intercambiar las columnas por las filas (i.e. pivotar los datos)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28e614df-51d8-41a1-8cca-109e2add0f70"}}},{"cell_type":"code","source":["spark.sql(\"\"\"\nSELECT destination, CAST(SUBSTRING(date,0,2) AS int) AS month, delay\nFROM departureDelays\nWHERE origin='SEA'\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"454f7bb1-3a29-4e20-b19b-a1d62c90d60d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------+-----+-----+\n|destination|month|delay|\n+-----------+-----+-----+\n|        ORD|    1|   92|\n|        JFK|    1|   -7|\n|        DFW|    1|   -5|\n|        MIA|    1|   -3|\n|        DFW|    1|   -3|\n|        DFW|    1|    1|\n|        ORD|    1|  -10|\n|        DFW|    1|   -6|\n|        DFW|    1|   -2|\n|        ORD|    1|   -3|\n|        ORD|    1|    0|\n|        DFW|    1|   23|\n|        DFW|    1|   36|\n|        ORD|    1|  298|\n|        JFK|    1|    4|\n|        DFW|    1|    0|\n|        MIA|    1|    2|\n|        DFW|    1|    0|\n|        DFW|    1|    0|\n|        ORD|    1|   83|\n+-----------+-----+-----+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------+-----+-----+\n|destination|month|delay|\n+-----------+-----+-----+\n|        ORD|    1|   92|\n|        JFK|    1|   -7|\n|        DFW|    1|   -5|\n|        MIA|    1|   -3|\n|        DFW|    1|   -3|\n|        DFW|    1|    1|\n|        ORD|    1|  -10|\n|        DFW|    1|   -6|\n|        DFW|    1|   -2|\n|        ORD|    1|   -3|\n|        ORD|    1|    0|\n|        DFW|    1|   23|\n|        DFW|    1|   36|\n|        ORD|    1|  298|\n|        JFK|    1|    4|\n|        DFW|    1|    0|\n|        MIA|    1|    2|\n|        DFW|    1|    0|\n|        DFW|    1|    0|\n|        ORD|    1|   83|\n+-----------+-----+-----+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Pivotar nos permite poner nombres en la columna month así como ejecutar cálculos de agregación (avg y max, por ejemplo)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87c7b40e-46b7-40df-99a8-59857667fbd8"}}},{"cell_type":"code","source":["spark.sql(\"\"\"\nSELECT * FROM (\nSELECT destination, CAST(SUBSTRING(date,0,2) AS int) AS month, delay\n    FROM departureDelays WHERE origin='SEA'\n)\nPIVOT (\nCAST(AVG(delay) AS DECIMAL(4,2)) AS AvgDelay, MAX(delay) AS MaxDelay\nFOR month IN (1 JAN, 2 FEB)\n)\nORDER BY destination\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07b0d74c-d2ed-498b-a64e-2d45e3c9becf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------+------------+------------+------------+------------+\n|destination|JAN_AvgDelay|JAN_MaxDelay|FEB_AvgDelay|FEB_MaxDelay|\n+-----------+------------+------------+------------+------------+\n|        ABQ|       19.86|         316|       11.42|          69|\n|        ANC|        4.44|         149|        7.90|         141|\n|        ATL|       11.98|         397|        7.73|         145|\n|        AUS|        3.48|          50|       -0.21|          18|\n|        BOS|        7.84|         110|       14.58|         152|\n|        BUR|       -2.03|          56|       -1.89|          78|\n|        CLE|       16.00|          27|        null|        null|\n|        CLT|        2.53|          41|       12.96|         228|\n|        COS|        5.32|          82|       12.18|         203|\n|        CVG|       -0.50|           4|        null|        null|\n|        DCA|       -1.15|          50|        0.07|          34|\n|        DEN|       13.13|         425|       12.95|         625|\n|        DFW|        7.95|         247|       12.57|         356|\n|        DTW|        9.18|         107|        3.47|          77|\n|        EWR|        9.63|         236|        5.20|         212|\n|        FAI|        1.84|         160|        4.21|          60|\n|        FAT|        1.36|         119|        5.22|         232|\n|        FLL|        2.94|          54|        3.50|          40|\n|        GEG|        2.28|          63|        2.87|          60|\n|        HDN|       -0.44|          27|       -6.50|           0|\n+-----------+------------+------------+------------+------------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------+------------+------------+------------+------------+\n|destination|JAN_AvgDelay|JAN_MaxDelay|FEB_AvgDelay|FEB_MaxDelay|\n+-----------+------------+------------+------------+------------+\n|        ABQ|       19.86|         316|       11.42|          69|\n|        ANC|        4.44|         149|        7.90|         141|\n|        ATL|       11.98|         397|        7.73|         145|\n|        AUS|        3.48|          50|       -0.21|          18|\n|        BOS|        7.84|         110|       14.58|         152|\n|        BUR|       -2.03|          56|       -1.89|          78|\n|        CLE|       16.00|          27|        null|        null|\n|        CLT|        2.53|          41|       12.96|         228|\n|        COS|        5.32|          82|       12.18|         203|\n|        CVG|       -0.50|           4|        null|        null|\n|        DCA|       -1.15|          50|        0.07|          34|\n|        DEN|       13.13|         425|       12.95|         625|\n|        DFW|        7.95|         247|       12.57|         356|\n|        DTW|        9.18|         107|        3.47|          77|\n|        EWR|        9.63|         236|        5.20|         212|\n|        FAI|        1.84|         160|        4.21|          60|\n|        FAT|        1.36|         119|        5.22|         232|\n|        FLL|        2.94|          54|        3.50|          40|\n|        GEG|        2.28|          63|        2.87|          60|\n|        HDN|       -0.44|          27|       -6.50|           0|\n+-----------+------------+------------+------------+------------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"CAP5_Python","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3058317980583756}},"nbformat":4,"nbformat_minor":0}
